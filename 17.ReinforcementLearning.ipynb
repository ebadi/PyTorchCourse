{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28ac930d",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "\n",
    "- SL (supervised learning) is like a static function\n",
    "- RL is more like a loop\n",
    "- RL we have a future plan\n",
    "- RL uses goals rather than targets (correct labels)\n",
    "- Agents observe Environment\n",
    "- Episode, sequence of events, rounds. No relationship between the episodes\n",
    "- State, Action, Rewards : Reward to each state and goal is to maximize the reward (+ve, -ve, 0)\n",
    "- The state can be derived from current and past observation\n",
    "- DQN: used 4 consequetive frame to represent the state\n",
    "- Policy yeild an action given ONLY the CURRENT state\n",
    "- Policy does NOT use the combination of past states and any rewards\n",
    "- Policy are usualy represented as probability\n",
    "- epsilon greedy: picking random action with epsilon rpobability\n",
    "- \"Agent\" Policy = Pi(a|s) a: action , s: state = softmax(W^T s)  W: policy parameter, shape: D x |A|\n",
    "- Define a problem using a framework to find a solution\n",
    "- Markov assumption, next state only depends ONLY on the current state at time t, and not the states before at t-1.\n",
    "- MDP: Markov Decision Process: state transistion probability of \"environment\" p(s', r | s, a), s' new state, r : reward, s : state, a : action\n",
    "- Policy is for environment, MDP is for environment dynamics\n",
    "- agent tries to maximize the total future reward , sum of future reward = RETURN or utility\n",
    "- For infinite horizon MDP, we use discounting rate gamma close to one like 0.9\n",
    "- Return(t) = Reward(t+a) + gamma* Return(t+1)\n",
    "- the VALUE function in RL the Expected return \n",
    "- exptected value in term of probability distribution is bellman equation\n",
    "- V_pi(S) : value fucntiuon for the policy pi, finding value function is called prediction\n",
    "\n",
    "\n",
    "Tasks in RL(MDP, markov decision process structure):\n",
    "- prediction problem : Given pi, find the corresponing value fucntion V(s)\n",
    "- Control problem: Find the optimal policy pi*, which yeild the maximum V(s)\n",
    "\n",
    "value functions:\n",
    "- V(s) : state value function\n",
    "- Q(s,a): action value function\n",
    "\n",
    "Best policy and best value function\n",
    "- V*(s)\n",
    "- Pi*(s)\n",
    "\n",
    "Monte carlo using sampling to find probabilities\n",
    "\n",
    "- g_t = r_t+1 + gamma * g_t+1\n",
    "\n",
    "```\n",
    "state, reward = play_episode_using_policy(policy)\n",
    "\n",
    "returns.append(g)\n",
    "for r in reversed(rewards):\n",
    "    g = t + gamma*g\n",
    "    returns.append(g)\n",
    "    \n",
    "returns = reversed(returns)\n",
    "```\n",
    "\n",
    "Prediction vs Control\n",
    "- prediction we are interested in V(s)\n",
    "- for control we want Q(s,a)\n",
    "\n",
    "Policy iteration and Policy improvement in loop:\n",
    "- Find Q(s,a) given pi (iteration)\n",
    "- Find pi as the argmax over Q(s,a)  (improvement)\n",
    "\n",
    "To solve exploit- explore delimma  we use Epsilon Greedy to do random actions \n",
    "\n",
    "### Q learning\n",
    "\n",
    "- Monte Carlo has a limitation that in roder to calculate returns, we need to wait until the episode ends (not ideal for problem with long or no terminal state)\n",
    "- Monte carlo using sampling as an apporximation to find probabilities and expected values\n",
    "- Temporal Difference (TD) is an approximation to Monty Carlo (an approximation of an approximation)\n",
    "- We wait one step to update the model, instead of waiting until the episode ends. boostrapped estimate of the return. \n",
    "- RL becomes similar to supervised learning, except the target is partly a model estimate\n",
    "- semi gradient : Gradient decent on the Q table\n",
    "\n",
    "### Deep Q-learning\n",
    "\n",
    "- Q learning with NN\n",
    "- Q table is a tabular method, we cannot have this table in a realistic situation unless we use binning (discrete categories)\n",
    "- approxiamtion methids as opposed to tabular method\n",
    "- DQN: Deep Q learning, the state space is infinite but the action space is discrete\n",
    "- Instead of updating V(s) we update the weight and bias vectors\n",
    "- Target : expecation of return \n",
    "- Prediction is V(s) = W^Ts + b\n",
    "- We want to update the linear regression parameter W and b\n",
    "- Stocastic Gradient Descent (SGD): One sample at time\n",
    "- Batch Gradient: Multiple samples at a time, descent is more stable\n",
    "- Replay buffer, populate trasition to the end of a buffer and use them as a batch to have stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ada92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "python rl_trader.py  -m train\n",
    "python rl_trader.py  -m test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
